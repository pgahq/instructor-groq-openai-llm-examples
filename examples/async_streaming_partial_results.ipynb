{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChQ4YaZHvYbR"
      },
      "source": [
        "## Summary\n",
        "\n",
        "Repo: https://github.com/pgahq/instructor-groq-openai-llm-examples\n",
        "\n",
        "This notebook shows how to do streaming responses with Instructor. It's also notable that this combo (LLM + Instructor) can create structured synthetic data.\n",
        "\n",
        "Note: this notebook assumes you're using Google Colab. You can safely edit / play here. Or go to `File` -> `Save a copy in Google Drive` to make your own version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7j-wjRULjwc4"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet instructor groq openai jsonref"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tA_GryIio4JN"
      },
      "source": [
        "On the left, click the key and set two secrets with your keys. Be sure to enable \"Notebook access\" for them. This is how Google Colab works...you're not sharing your keys with anyone.\n",
        "\n",
        "OPENAI_API_KEY\n",
        "\n",
        "GROQ_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M9Om0C2YmGGj"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import groq\n",
        "import instructor\n",
        "from pydantic import BaseModel, Field\n",
        "import os\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    os.environ['OPENAI_API_KEY'] = '' or userdata.get('OPENAI_API_KEY') # or put your key in the '' on this line\n",
        "    os.environ['GROQ_API_KEY'] = '' or userdata.get('GROQ_API_KEY')\n",
        "except Exception as e:\n",
        "    # print(e)\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2xtRfmD6pgH"
      },
      "source": [
        "Now to the cool stuff..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50rSTdEq6nr6",
        "outputId": "0672a4c9-be13-4ae2-c6ab-a55cc0204fb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "name: Alex Johnson\n",
            "name: Alex Johnson, age: 28\n",
            "name: Alex Johnson, age: 28, gender: Non-Binary\n",
            "name: Alex Johnson, age: 28, gender: Non-Binary, city: Denver\n",
            "name: Alex Johnson, age: 28, gender: Non-Binary, city: Denver, state: Colorado\n",
            "name: Alex Johnson, age: 28, gender: Non-Binary, city: Denver, state: Colorado, country: USA\n",
            "name: Alex Johnson, age: 28, gender: Non-Binary, city: Denver, state: Colorado, country: USA, backstory: Alex grew up in a small town but moved to Denver to pursue a career in tech. Enthusiastic about coding and passionate about community service, Alex spends weekends volunteering at local shelters and mentoring underprivileged youth in coding.\n"
          ]
        }
      ],
      "source": [
        "inference_provider = \"openai\"   # \"openai\" or \"groq\"\n",
        "client = instructor.from_openai(openai.OpenAI()) if inference_provider == \"openai\" else instructor.from_groq(groq.Groq())\n",
        "\n",
        "class User(BaseModel):\n",
        "    name: str = Field(description=\"The name of the user\")\n",
        "    age: int = Field(description=\"The age of the user\")\n",
        "    gender: str = Field(description=\"The gender of the user\")\n",
        "    city: str = Field(description=\"The city of the user\")\n",
        "    state: str = Field(description=\"The state of the user\")\n",
        "    country: str = Field(description=\"The country of the user\")\n",
        "    backstory: str = Field(description=\"A backstory about the user\")\n",
        "\n",
        "user_stream = client.chat.completions.create_partial(   # create_partial is what does the asynchronous streaming (usually it's \"create\")\n",
        "    model=\"gpt-4o\" if inference_provider == \"openai\" else \"llama3-70b-8192\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Create 1 synthetic user\"},   # ask the LLM to make up some data\n",
        "    ],\n",
        "    response_model=User,\n",
        ")\n",
        "\n",
        "# below is some overly complicated code to print the results...really just a prettier version of:\n",
        "# for user in user_stream:\n",
        "#     print(user)\n",
        "\n",
        "previous_output = None\n",
        "for user in user_stream:\n",
        "    # this loops with every token\n",
        "    user_dict = user.model_dump()\n",
        "    current_output = \", \".join(f\"{key}: {value}\" for key, value in user_dict.items() if value is not None)\n",
        "    if current_output != previous_output:\n",
        "        print(current_output)   # only print when there's a complete new value i.e. all tokens have arrived\n",
        "        previous_output = current_output\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
