{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChQ4YaZHvYbR"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Repo: https://github.com/pgahq/instructor-groq-openai-llm-examples\n",
        "\n",
        "This notebook shows how to use Instructor with LLMs to keep things clean and avoid having to write/generate the messy function schema.\n",
        "\n",
        "The magic comes from `OpenAISchema` in Instructor. Each tool is defined like this:\n",
        "\n",
        "```\n",
        "class MyTool(OpenAISchema):\n",
        "    \"\"\"\n",
        "    Detailed description here tells the LLM when and how to use this tool.\n",
        "    \"\"\"\n",
        "    param1: str = Field(..., description=\"describe this param\")\n",
        "    param2: int = Field(..., description=\"describe this param\")\n",
        "\n",
        "    def run(self):\n",
        "      # the actual tool code goes here\n",
        "      return(results from the tool)\n",
        "```\n",
        "\n",
        "Note: this notebook assumes you're using Google Colab. You can safely edit / play here. Or go to `File` -> `Save a copy in Google Drive` to make your own version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7j-wjRULjwc4"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install instructor groq openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tA_GryIio4JN"
      },
      "source": [
        "On the left, click the key and set two secrets with your keys. Be sure to enable \"Notebook access\" for them. This is how Google Colab works...you're not sharing your keys with anyone.\n",
        "\n",
        "OPENAI_API_KEY\n",
        "\n",
        "GROQ_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M9Om0C2YmGGj"
      },
      "outputs": [],
      "source": [
        "from instructor import OpenAISchema   # this is what makes everything possible\n",
        "import openai\n",
        "import groq\n",
        "from pydantic import BaseModel, Field, field_validator\n",
        "from typing import List\n",
        "import subprocess\n",
        "from rich import print as rprint\n",
        "import os\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    os.environ['OPENAI_API_KEY'] = '' or userdata.get('OPENAI_API_KEY') # or put your key in the '' on this line\n",
        "    os.environ['GROQ_API_KEY'] = '' or userdata.get('GROQ_API_KEY')\n",
        "except Exception as e:\n",
        "    # print(e)\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2xtRfmD6pgH"
      },
      "source": [
        "A couple boring util functions that will run the tools we'll define later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "50rSTdEq6nr6"
      },
      "outputs": [],
      "source": [
        "def get_completion_and_use_tools(messages, tool_functions, client, model):\n",
        "    while True:   # loop until the requested tool(s) has been used and/or a normal text response has been provided by the LLM\n",
        "        completion = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=messages,\n",
        "            tools=[{\"type\": \"function\", \"function\": func.openai_schema} for func in tool_functions],   # func.openai_schema is the hero here\n",
        "            tool_choice=\"auto\",\n",
        "            temperature=0.5,\n",
        "            max_tokens=4096,\n",
        "        )\n",
        "\n",
        "        completion_message = completion.choices[0].message\n",
        "        if completion_message.tool_calls is None:\n",
        "            return(completion_message.content)  # no tool...just a regular LLM response\n",
        "        else:\n",
        "            messages.append(completion_message)   # chat completions requires this\n",
        "            for tool_call in completion_message.tool_calls:\n",
        "                print(f\"\\033[90müõ†Ô∏è {tool_call.function.name} {tool_call.function.arguments}\\033[0m\")\n",
        "                tool_result = execute_tool(tool_call, tool_functions)   # run the requested tool\n",
        "                messages.append({   # chat completions requires this\n",
        "                    \"tool_call_id\": tool_call.id,\n",
        "                    \"role\": \"tool\",\n",
        "                    \"name\": tool_call.function.name,\n",
        "                    \"content\": tool_result,\n",
        "                    })\n",
        "\n",
        "def execute_tool(tool_call, funcs):\n",
        "    # inspired by https://github.com/VRSEN/agency-swarm/threads/thread.py - original source?\n",
        "    func = next(iter([func for func in funcs if func.__name__ == tool_call.function.name]))\n",
        "\n",
        "    if not func:\n",
        "        return f\"Error: Function {tool_call.function.name} not found. Available functions: {[func.__name__ for func in funcs]}\"\n",
        "    try:\n",
        "        func = func(**eval(tool_call.function.arguments))\n",
        "        output = func.run()  # always execute only the run() function from the tool class\n",
        "        return output\n",
        "    except Exception as e:\n",
        "        return \"Error: \" + str(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J03ZkuhTG8PE"
      },
      "source": [
        "## Create the tools\n",
        "With Instructor, tools are classes derived from `OpenAISchema`. When a tool gets called, its `run()` function will be executed (because that's how `execute_tool()` is defined above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1RfbVKfGBpmi"
      },
      "outputs": [],
      "source": [
        "class GetWeather(OpenAISchema):\n",
        "    \"\"\"\n",
        "    Determine weather in a location\n",
        "\n",
        "    ## Limitations\n",
        "    - Only returns temperature in fahrenheit\n",
        "    \"\"\"\n",
        "    location: str = Field(..., description=\"The city and state e.g. San Francisco, CA\")\n",
        "\n",
        "    def run(self):      # implement the tool code here\n",
        "      # this example tool implementation is hard coded with no actual logic\n",
        "      return(f\"Rainy, 48 degrees fahrenheit in {self.location}\")\n",
        "\n",
        "class GetDirections(OpenAISchema):\n",
        "    \"\"\"\n",
        "    Get driving directions from one city to another.\n",
        "\n",
        "    ## Limitations\n",
        "    - Distances are only given in miles\n",
        "    \"\"\"\n",
        "    from_city: str = Field(..., description=\"The name of the departure city\")\n",
        "    to_city: str = Field(..., description=\"The name of the destination city\")\n",
        "\n",
        "    def run(self):      # implement the tool code here\n",
        "      # this example tool implementation is hard coded with no actual logic\n",
        "      return(f\"Drive north to get from {self.from_city} to {self.to_city}\")\n",
        "\n",
        "# collect the tool classes we just defined into an array so they can be used as tools later\n",
        "tool_functions = [GetWeather, GetDirections]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PqEpzaPLq2T"
      },
      "source": [
        "## Let it fly\n",
        "Pass in the prompt and the tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amp2qAEzDUYW",
        "outputId": "2754147f-7997-4dd1-bbaf-e3fe2f42e186"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gpt-4-turbo via OpenAI:\n",
            "\u001b[90müõ†Ô∏è GetWeather {\"location\": \"New York, NY\"}\u001b[0m\n",
            "\u001b[90müõ†Ô∏è GetDirections {\"from_city\": \"Florida\", \"to_city\": \"New York, NY\"}\u001b[0m\n",
            "The weather in New York, NY is rainy with a temperature of 48¬∞F. To get there from Florida, drive north.\n",
            "\n",
            "llama3-70b-8192 via Groq:\n",
            "\u001b[90müõ†Ô∏è GetWeather {\"location\":\"New York City, NY\"}\u001b[0m\n",
            "\u001b[90müõ†Ô∏è GetDirections {\"from_city\":\"Florida\",\"to_city\":\"New York City\"}\u001b[0m\n",
            "Since you're planning to drive from Florida to New York City, I'll provide you with a more detailed response. Here's a concise answer: \"To get to New York City from Florida, drive north. The current weather in NYC is rainy with a temperature of 48 degrees Fahrenheit.\"\n",
            "\n"
          ]
        }
      ],
      "source": [
        "messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You answer questions - short and sweet.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What's the weather in NYC and how do I get there from Florida?\"},    # the main prompt\n",
        "]\n",
        "\n",
        "print(\"gpt-4o via OpenAI:\")\n",
        "result = get_completion_and_use_tools(\n",
        "            messages.copy(), # or pass messages directly to retain the tool calling chatter in the thread\n",
        "            tool_functions,  # the array containing the OpenAISchema tool classes defined earlier\n",
        "            openai.OpenAI(), # standard stuff\n",
        "            \"gpt-4o\"    # model name\n",
        "            )\n",
        "print(result + \"\\n\")\n",
        "\n",
        "print(\"llama3-70b-8192 via Groq:\")\n",
        "print(get_completion_and_use_tools(messages.copy(), tool_functions, groq.Groq(), \"llama3-70b-8192\") + \"\\n\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
