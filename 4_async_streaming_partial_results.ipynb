{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChQ4YaZHvYbR"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook shows how to do streaming responses with Instructor. It's also notable that this combo (LLM + Instructor) can create structured synthetic data.\n",
        "\n",
        "Note: this notebook assumes you're using Google Colab. You can safely edit / play here. Or go to `File` -> `Save a copy in Google Drive` to make your own version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7j-wjRULjwc4"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install instructor groq openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tA_GryIio4JN"
      },
      "source": [
        "On the left, click the key and set two secrets with your keys. Be sure to enable \"Notebook access\" for them. This is how Google Colab works...you're not sharing your keys with anyone.\n",
        "\n",
        "OPENAI_API_KEY\n",
        "\n",
        "GROQ_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M9Om0C2YmGGj"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import groq\n",
        "import instructor\n",
        "from pydantic import BaseModel, Field\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = '' or userdata.get('OPENAI_API_KEY') # or put your key in the '' on this line\n",
        "os.environ['GROQ_API_KEY'] = '' or userdata.get('GROQ_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2xtRfmD6pgH"
      },
      "source": [
        "Now to the cool stuff..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50rSTdEq6nr6",
        "outputId": "0672a4c9-be13-4ae2-c6ab-a55cc0204fb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "name: Jake Stevens\n",
            "name: Jake Stevens, age: 34\n",
            "name: Jake Stevens, age: 34, gender: Male\n",
            "name: Jake Stevens, age: 34, gender: Male, city: Austin\n",
            "name: Jake Stevens, age: 34, gender: Male, city: Austin, state: Texas\n",
            "name: Jake Stevens, age: 34, gender: Male, city: Austin, state: Texas, country: USA\n",
            "name: Jake Stevens, age: 34, gender: Male, city: Austin, state: Texas, country: USA, backstory: Jake is a software developer living in Austin, Texas. He loves hiking and exploring the outdoors, often taking trips to the nearby Hill Country. Passionate about technology and innovation, Jake spends his spare time working on personal coding projects and contributing to open source communities.\n"
          ]
        }
      ],
      "source": [
        "inference_provider = \"openai\"   # \"openai\" or \"groq\"\n",
        "client = instructor.from_openai(openai.OpenAI()) if inference_provider == \"openai\" else instructor.from_groq(groq.Groq())\n",
        "\n",
        "class User(BaseModel):\n",
        "    name: str = Field(description=\"The name of the user\")\n",
        "    age: int = Field(description=\"The age of the user\")\n",
        "    gender: str = Field(description=\"The gender of the user\")\n",
        "    city: str = Field(description=\"The city of the user\")\n",
        "    state: str = Field(description=\"The state of the user\")\n",
        "    country: str = Field(description=\"The country of the user\")\n",
        "    backstory: str = Field(description=\"A backstory about the user\")\n",
        "\n",
        "user_stream = client.chat.completions.create_partial(   # create_partial is what does the asynchronous streaming (usually it's \"create\")\n",
        "    model=\"gpt-4-turbo\" if inference_provider == \"openai\" else \"llama3-70b-8192\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Create 1 synthetic user\"},   # ask the LLM to make up some data\n",
        "    ],\n",
        "    response_model=User,\n",
        ")\n",
        "\n",
        "# below is some overly complicated code to print the results...really just a prettier version of:\n",
        "# for user in user_stream:\n",
        "#     print(user)\n",
        "\n",
        "previous_output = None\n",
        "for user in user_stream:\n",
        "    # this loops with every token\n",
        "    user_dict = user.model_dump()\n",
        "    current_output = \", \".join(f\"{key}: {value}\" for key, value in user_dict.items() if value is not None)\n",
        "    if current_output != previous_output:\n",
        "        print(current_output)   # only print when there's a complete new value i.e. all tokens have arrived\n",
        "        previous_output = current_output\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
